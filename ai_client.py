import os
import requests
import time
import logging
from dotenv import load_dotenv
from typing import Optional, Dict, Any
# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# Add this to the top of ai_client.py
try:
    from secrets_config import LLM_PROVIDER, OPENROUTER_KEY, GEMINI_KEY, GROQ_KEY
except ImportError:
    # Fallback for local development
    from dotenv import load_dotenv
    import os
    load_dotenv()
    LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openrouter").lower()
    OPENROUTER_KEY = os.getenv("OPENROUTER_API_KEY")
    GEMINI_KEY = os.getenv("GEMINI_API_KEY")
    GROQ_KEY = os.getenv("GROQ_API_KEY")

# Request timeout and retry settings
DEFAULT_TIMEOUT = 30
MAX_RETRIES = 3
RETRY_DELAY = 2

class LLMError(Exception):
    """Custom exception for LLM-related errors"""
    pass

def validate_api_keys():
    """Validate that required API keys are present"""
    key_mapping = {
        "openrouter": OPENROUTER_KEY,
        "gemini": GEMINI_KEY,
        "groq": GROQ_KEY
    }
    
    if LLM_PROVIDER not in key_mapping:
        raise LLMError(f"Unknown LLM provider: {LLM_PROVIDER}")
    
    if not key_mapping[LLM_PROVIDER]:
        raise LLMError(f"API key for {LLM_PROVIDER} is not set")
    
    return True

def handle_request_errors(func):
    """Decorator to handle common request errors with retry logic"""
    def wrapper(*args, **kwargs):
        for attempt in range(MAX_RETRIES):
            try:
                return func(*args, **kwargs)
            except requests.exceptions.Timeout:
                logger.warning(f"Request timeout, attempt {attempt + 1}/{MAX_RETRIES}")
                if attempt == MAX_RETRIES - 1:
                    raise LLMError("Request timed out after multiple attempts")
                time.sleep(RETRY_DELAY * (attempt + 1))
            except requests.exceptions.ConnectionError:
                logger.warning(f"Connection error, attempt {attempt + 1}/{MAX_RETRIES}")
                if attempt == MAX_RETRIES - 1:
                    raise LLMError("Failed to connect to the API server")
                time.sleep(RETRY_DELAY * (attempt + 1))
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 429:  # Rate limit
                    logger.warning(f"Rate limited, attempt {attempt + 1}/{MAX_RETRIES}")
                    if attempt == MAX_RETRIES - 1:
                        raise LLMError("Rate limit exceeded. Please try again later.")
                    time.sleep(RETRY_DELAY * (attempt + 2))  # Longer delay for rate limits
                elif e.response.status_code >= 500:  # Server errors
                    logger.warning(f"Server error {e.response.status_code}, attempt {attempt + 1}/{MAX_RETRIES}")
                    if attempt == MAX_RETRIES - 1:
                        raise LLMError(f"Server error: {e.response.status_code}")
                    time.sleep(RETRY_DELAY * (attempt + 1))
                else:
                    # Client errors (4xx) - don't retry
                    raise LLMError(f"API error: {e.response.status_code} - {e.response.text}")
            except Exception as e:
                logger.error(f"Unexpected error: {str(e)}")
                raise LLMError(f"Unexpected error: {str(e)}")
    return wrapper

@handle_request_errors
def call_openrouter(prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:
    """
    Calls OpenRouter API for chat completions with enhanced error handling.
    """
    if not OPENROUTER_KEY:
        raise LLMError("OPENROUTER_API_KEY is not set")

    url = "https://openrouter.ai/api/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENROUTER_KEY}",
        "Content-Type": "application/json",
        "HTTP-Referer": "https://hybrid-quantum-ai.local",  # Required by OpenRouter
        "X-Title": "Hybrid Quantum AI Chatbot"
    }
    
    payload = {
        "model": "openai/gpt-3.5-turbo",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
        "frequency_penalty": 0.1,
        "presence_penalty": 0.1
    }

    response = requests.post(url, json=payload, headers=headers, timeout=DEFAULT_TIMEOUT)
    response.raise_for_status()
    
    data = response.json()
    
    if "error" in data:
        raise LLMError(f"OpenRouter API error: {data['error']}")
    
    if not data.get("choices") or not data["choices"]:
        raise LLMError("No response generated by OpenRouter")
    
    return data["choices"][0]["message"]["content"].strip()

@handle_request_errors
def call_gemini(prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:
    """
    Calls Gemini (Google Generative Language API) with enhanced error handling.
    """
    if not GEMINI_KEY:
        raise LLMError("GEMINI_API_KEY is not set")

    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-pro:generateContent?key={GEMINI_KEY}"
    headers = {"Content-Type": "application/json"}
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {
            "maxOutputTokens": max_tokens,
            "temperature": temperature,
            "topP": 0.8,
            "topK": 10
        },
        "safetySettings": [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_MEDIUM_AND_ABOVE"
            }
        ]
    }

    response = requests.post(url, json=payload, headers=headers, timeout=DEFAULT_TIMEOUT)
    response.raise_for_status()
    
    data = response.json()
    
    if "error" in data:
        raise LLMError(f"Gemini API error: {data['error']}")
    
    try:
        content = data["candidates"][0]["content"]["parts"][0]["text"]
        return content.strip()
    except (KeyError, IndexError) as e:
        logger.error(f"Gemini response parsing error: {e}, response: {data}")
        raise LLMError(f"Failed to parse Gemini response: {str(e)}")

@handle_request_errors
def call_groq(prompt: str, max_tokens: int = 512, temperature: float = 0.7) -> str:
    """
    Calls Groq API (OpenAI-compatible) with enhanced error handling.
    """
    if not GROQ_KEY:
        raise LLMError("GROQ_API_KEY is not set")

    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_KEY}",
        "Content-Type": "application/json",
    }
    
    payload = {
        "model": "mixtral-8x7b-32768",
        "messages": [{"role": "user", "content": prompt}],
        "max_tokens": max_tokens,
        "temperature": temperature,
        "top_p": 0.9,
        "stream": False
    }

    response = requests.post(url, json=payload, headers=headers, timeout=DEFAULT_TIMEOUT)
    response.raise_for_status()
    
    data = response.json()
    
    if "error" in data:
        raise LLMError(f"Groq API error: {data['error']}")
    
    if not data.get("choices") or not data["choices"]:
        raise LLMError("No response generated by Groq")
    
    return data["choices"][0]["message"]["content"].strip()

def quantum_fallback_response() -> str:
    """Generate a quantum-themed fallback response when LLM fails"""
    import random
    
    fallback_responses = [
        "The quantum superposition of possibilities suggests multiple valid approaches to your question.",
        "Due to quantum uncertainty, I can offer that your query exists in a state of potential solutions.",
        "Quantum entanglement with the information field indicates interconnected pathways to your answer.",
        "In quantum terms, your question has collapsed the wave function into a need for further exploration.",
        "The quantum measurement of available knowledge returns: 'Please rephrase for optimal coherence.'"
    ]
    
    return random.choice(fallback_responses)

def ask_llm(prompt: str, max_tokens: int = 512, temperature: float = 0.7, 
           use_fallback: bool = True) -> str:
    """
    Routes the prompt to the configured LLM provider with comprehensive error handling.
    
    Args:
        prompt: The input prompt
        max_tokens: Maximum tokens to generate
        temperature: Sampling temperature
        use_fallback: Whether to use quantum fallback on complete failure
    
    Returns:
        Generated response string
    
    Raises:
        LLMError: When all attempts fail and fallback is disabled
    """
    try:
        # Validate configuration
        validate_api_keys()
        
        # Input validation
        if not prompt or not prompt.strip():
            return "I need a question or statement to respond to. Please try again."
        
        if len(prompt) > 8000:  # Reasonable limit
            prompt = prompt[:8000] + "... [truncated]"
        
        # Route to appropriate provider
        logger.info(f"Calling {LLM_PROVIDER} with prompt length: {len(prompt)}")
        
        if LLM_PROVIDER == "openrouter":
            response = call_openrouter(prompt, max_tokens, temperature)
        elif LLM_PROVIDER == "gemini":
            response = call_gemini(prompt, max_tokens, temperature)
        elif LLM_PROVIDER == "groq":
            response = call_groq(prompt, max_tokens, temperature)
        else:
            raise LLMError(f"Unknown LLM provider: {LLM_PROVIDER}")
        
        # Validate response
        if not response or not response.strip():
            if use_fallback:
                return quantum_fallback_response()
            else:
                raise LLMError("Empty response from LLM")
        
        logger.info(f"Successfully generated response of length: {len(response)}")
        return response
        
    except LLMError:
        # Re-raise LLM errors
        raise
    except Exception as e:
        logger.error(f"Unexpected error in ask_llm: {str(e)}")
        if use_fallback:
            return quantum_fallback_response()
        else:
            raise LLMError(f"Unexpected error: {str(e)}")

def get_provider_status() -> Dict[str, Any]:
    """Get status information about the current LLM provider"""
    try:
        validate_api_keys()
        return {
            "provider": LLM_PROVIDER,
            "status": "configured",
            "key_present": True,
            "error": None
        }
    except LLMError as e:
        return {
            "provider": LLM_PROVIDER,
            "status": "error",
            "key_present": False,
            "error": str(e)
        }